# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uHmVMBZRNWbr6FHwHXOR0fSzyc_Jtke2
"""

import pandas as pd
import numpy as np
from sklearn.utils import shuffle
import tensorflow as tf
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split


monday = pd.read_csv('Monday-WorkingHours.pcap_ISCX.csv')
tuesday = pd.read_csv('Tuesday-WorkingHours.pcap_ISCX.csv')
wednesday = pd.read_csv('Wednesday-workingHours.pcap_ISCX.csv')
thursday_morning = pd.read_csv('Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')
thursday_afternoon = pd.read_csv('Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')
friday_morning = pd.read_csv('Friday-WorkingHours-Morning.pcap_ISCX.csv')
friday = pd.read_csv('Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')


monday = monday.dropna()
tuesday = tuesday.dropna()
wednesday = wednesday.dropna()
thursday_morning = thursday_morning.dropna()
thursday_afternoon = thursday_afternoon.dropna()
friday_morning = friday_morning.dropna()
friday = friday.dropna()

# clean data
convert_dict = {' Subflow Bwd Packets': int} 
monday = monday.astype(convert_dict)

days = pd.concat([monday, tuesday, wednesday, thursday_morning, thursday_afternoon, friday_morning, friday])

column_names = list(days.columns.values)
processed_names = []
for name in column_names:
    processed_names.append(name.strip())


days.columns = processed_names

convert_dict = {'Flow Packets/s': np.float64,
                'Flow Bytes/s': np.float64} 


days = days.astype(convert_dict)

days = days.replace([np.inf], np.nan).dropna()

days = days.drop(['Bwd PSH Flags', 'Bwd URG Flags', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 
                  'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate'], axis=1)

days = shuffle(days)

labels = days['Label']
days = days.drop('Label', axis=1)

labels[labels.str.contains('Web Attack')] = 'Web Attack'
labels[labels.str.contains('DoS')] = 'DoS'
labels[labels.str.contains('Heartbleed')] = 'DoS'
labels[labels.str.contains('BENIGN')] = 'Normal'
labels[labels.str.contains('Patator')] = 'Brute Force'

days=(days-days.mean())/(days.std())

ohe_labels = pd.get_dummies(list(labels))

X_train, X_test, y_train, y_test = train_test_split(days, ohe_labels, test_size = 0.1)

X_train_m = X_train.values
y_train_m = y_train.values

X_test_m = X_test.values
y_test_m = y_test.values

num_classes = y_train_m.shape[1]
n_samples = y_train_m.shape[0]
num_features = X_train_m.shape[1]

num_output = y_train_m.shape[1]
num_layers_0 = 50
regularizer_rate = 0.1

# Parameters
learning_rate = 0.001
training_epochs = 40
batch_size = 100
display_step = 1
regularizer_rate = 0.1

# Network Parameters
n_hidden_1 = 50
n_hidden_2 = 25 
n_input = num_features 
n_classes = num_classes


X = tf.placeholder("float", [None, n_input], name='X')
Y = tf.placeholder("float", [None, n_classes], name='Y')


# Store layers weight & bias
weights = {
    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=(1/tf.sqrt(float(num_features))))),
    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=(1/tf.sqrt(float(num_features))))),
    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], stddev=(1/tf.sqrt(float(num_features)))))
}
biases = {
    'b1': tf.Variable(tf.random_normal([n_hidden_1])),
    'b2': tf.Variable(tf.random_normal([n_hidden_2])),
    'out': tf.Variable(tf.random_normal([n_classes]))
}

# Create model
def multilayer_perceptron(x):
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])
    # Output fully connected layer with a neuron for each class
    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']
    return out_layer


logits = multilayer_perceptron(X)

loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(
    logits=logits, labels=Y)) + regularizer_rate*(tf.reduce_sum(tf.square(biases['b1'])))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
train_op = optimizer.minimize(loss_op)

init = tf.global_variables_initializer()

training_accuracy = []
training_loss = []
testing_accuracy = []

keep_prob = tf.placeholder(tf.float32)

correct_prediction = tf.equal(tf.argmax(y_train,1), tf.argmax(logits,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    sess.run(init)
    
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(n_samples/batch_size)
        for batch in range(total_batch):
            batch_x = X_train_m[batch*batch_size : (1+batch)*batch_size]
            batch_y = y_train_m[batch*batch_size : (1+batch)*batch_size]
    
            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,
                                                            Y: batch_y})
        
            avg_cost += c / total_batch
        
        
        training_accuracy.append(sess.run(accuracy, feed_dict= {X:X_train, 
                                                         Y: y_train
                                                         }))    
        training_loss.append(sess.run(loss_op, {X: X_train, 
                                      Y: y_train}))
    
        # Evaluation of model
        testing_accuracy.append(accuracy_score(y_test_m.argmax(1), 
                            sess.run(logits, {X: X_test_m}).argmax(1)))
        
        if epoch % display_step == 0:
            print("Epoch:{0}, Train loss: {1:.2f} Train acc: {2:.3f}, Test acc:{3:.3f}".format(epoch, training_loss[epoch], training_accuracy[epoch],testing_accuracy[epoch]))
            # print("Epoch:", '%04d' % (epoch+1), "cost={:.9f}".format(avg_cost))
    
    print("Optimization Finished!")
    
    # Test model
    pred = tf.nn.softmax(logits)  # Apply softmax to logits
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    print("Accuracy:", accuracy.eval({X: X_test_m, Y: y_test_m}))